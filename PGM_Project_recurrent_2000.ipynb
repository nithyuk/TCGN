{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "9Mdz6WLlM9uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-2.4.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "!pip install torch-geometric-temporal\n",
        "!pip install yfinance\n",
        "!pip install requests_html\n",
        "!pip install yahoo_fin"
      ],
      "metadata": {
        "id": "4JiVx2AQNAr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html"
      ],
      "metadata": {
        "id": "CYRYPni9NFed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy0yRjMOo8Cu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric_temporal.nn.recurrent import TGCN\n",
        "\n",
        "# This will error out, open the TGCN file and replace 'from torch_geometric.utils.to_dense_adj import to_dense_adj' with\n",
        "#                                                     'from torch_geometric.utils import to_dense_adj'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXzx39vZmLXA"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from tqdm import tqdm\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "KG57q-Q1NKhx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4t2XKcSrrGn"
      },
      "outputs": [],
      "source": [
        "# Load price data csv\n",
        "price_data = pd.read_csv(\"price_data.csv\", index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "hPlNhhkPOMY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb-sBlNsAiG8"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tC_h_GVlyIg"
      },
      "outputs": [],
      "source": [
        "def create_correlation_graph(price_data, corr_min, window_size=20):\n",
        "    \"\"\"\n",
        "    Create rolling correlation-based edge weights\n",
        "    \"\"\"\n",
        "    returns = price_data.pct_change()\n",
        "\n",
        "    # Calculate rolling correlation\n",
        "    corr_matrices = []\n",
        "    for i in range(len(returns) - window_size + 1):\n",
        "        window = returns.iloc[i:i+window_size]\n",
        "        corr = window.corr()\n",
        "        corr_matrices.append(corr)\n",
        "\n",
        "    # Create edges and weights\n",
        "    edge_index = []\n",
        "    edge_weights = []\n",
        "\n",
        "    for corr_matrix in tqdm(corr_matrices):\n",
        "        edges = np.argwhere(corr_matrix.to_numpy() > corr_min)\n",
        "\n",
        "        weights = corr_matrix.to_numpy()[edges[:, 0], edges[:, 1]]\n",
        "        edge_index.append(torch.tensor(edges).t().contiguous())\n",
        "        edge_weights.append(torch.tensor(weights))\n",
        "\n",
        "    return edge_index, edge_weights\n",
        "\n",
        "def prepare_stock_features(symbols, price_data, pos_bound, neg_bound, scaler=None, window_size=20):\n",
        "    \"\"\"Prepare stock features with technical indicators\"\"\"\n",
        "    exceeds_all = []\n",
        "    misses_all = []\n",
        "    # Calculate features\n",
        "    features = []\n",
        "    for symbol in symbols:\n",
        "        df = pd.DataFrame()\n",
        "        # Price momentum\n",
        "        df['returns'] = price_data[symbol].pct_change()\n",
        "        df['momentum'] = price_data[symbol].pct_change(window_size)\n",
        "        # Volatility\n",
        "        df['volatility'] = df['returns'].rolling(window=window_size).std()\n",
        "        # Moving averages\n",
        "        df['ma20'] = price_data[symbol].rolling(window=20).mean()\n",
        "        df['ma50'] = price_data[symbol].rolling(window=50).mean()\n",
        "\n",
        "        df['tern'] = 0\n",
        "        df.loc[df['returns'] > pos_bound * df['volatility'], 'tern'] = 1\n",
        "        df.loc[-df['returns'] > neg_bound * df['volatility'], 'tern'] = -1\n",
        "        exceeds_all.append(len(df[df.tern == 1]) / len(df))\n",
        "        misses_all.append(len(df[df.tern == -1]) / len(df))\n",
        "        features.append(df)\n",
        "\n",
        "    print(\"Exceeds Pricing: \", sum(exceeds_all) / len(exceeds_all))\n",
        "    print(\"Misses Pricing: \", sum(misses_all) / len(misses_all))\n",
        "\n",
        "    # Combine features\n",
        "    combined_features = np.stack([f.fillna(0).values for f in features], axis=1)\n",
        "    if scaler is None:\n",
        "      # Scale features\n",
        "      scaler = StandardScaler()\n",
        "      scaled_features = scaler.fit_transform(combined_features.reshape(-1, combined_features.shape[-1]))\n",
        "      scaled_features = scaled_features.reshape(combined_features.shape)\n",
        "    else:\n",
        "      scaled_features = scaler.transform(combined_features.reshape(-1, combined_features.shape[-1]))\n",
        "      scaled_features = scaled_features.reshape(combined_features.shape)\n",
        "    scaled_features[:, :, -1] = combined_features[:, :, -1]\n",
        "    return torch.tensor(scaled_features), scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeN2HHdoRc4b"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "split_date = price_data.index[-126]  # Approximately 126 business days for last 6 months\n",
        "price_data_train = price_data[price_data.index < split_date]\n",
        "price_data_test = price_data[price_data.index >= split_date]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7s9jsl9KTKv"
      },
      "outputs": [],
      "source": [
        "CORR_MIN = 0.4\n",
        "\n",
        "POSITIVE_BOUND = 0.4\n",
        "NEGATIVE_BOUND = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNeSXpaBIYt7",
        "outputId": "acde8c50-a311-4db0-c90c-b932514279ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing stock features...\n",
            "Exceeds Pricing:  0.31907777040744495\n",
            "Misses Pricing:  0.313248507507688\n"
          ]
        }
      ],
      "source": [
        "# Prepare Train Data\n",
        "print(\"Preparing training stock features...\")\n",
        "features_train, scaler_train = prepare_stock_features(price_data_train.columns, price_data_train, POSITIVE_BOUND, NEGATIVE_BOUND)\n",
        "features_train = features_train.to(torch.float32)\n",
        "\n",
        "print(\"Creating training correlation matrices...\")\n",
        "edge_index_train, edge_weights_train = create_correlation_graph(price_data_train, CORR_MIN)\n",
        "edge_index_train = [ei.to(torch.long) for ei in edge_index_train]\n",
        "edge_weights_train = [ew.to(torch.float32) for ew in edge_weights_train]\n",
        "\n",
        "# Clip excess train data\n",
        "features_train = features_train[-len(edge_index_train):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUSdu3TBKTKy",
        "outputId": "de3f0fc8-7aa4-4846-9a17-a184df52cd9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exceeds Pricing:  0.28134016891781355\n",
            "Misses Pricing:  0.24850307272667393\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 107/107 [00:04<00:00, 24.42it/s]\n"
          ]
        }
      ],
      "source": [
        "# Prepare Test Data\n",
        "print(\"Preparing testing stock features...\")\n",
        "features_test, _ = prepare_stock_features(price_data_test.columns, price_data_test, POSITIVE_BOUND, NEGATIVE_BOUND, scaler=scaler_train)\n",
        "features_test = features_test.to(torch.float32)\n",
        "\n",
        "print(\"Creating testing correlation matrices...\")\n",
        "edge_index_test, edge_weights_test = create_correlation_graph(price_data_test, CORR_MIN)\n",
        "edge_index_test = [ei.to(torch.long) for ei in edge_index_test]\n",
        "edge_weights_test = [ew.to(torch.float32) for ew in edge_weights_test]\n",
        "\n",
        "# Clip excess test data\n",
        "features_test = features_test[-len(edge_index_test):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5aTjNg2ndF5",
        "outputId": "1cc658f5-1d5a-4fdc-dcc1-f54fea2ccaa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1114"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(features_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "LSG10Q59OYEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR29Vgp6KTKy"
      },
      "outputs": [],
      "source": [
        "class TemporalStockPredictor(nn.Module):\n",
        "    def __init__(self, node_features, hidden_dim, num_heads=4):\n",
        "        super(TemporalStockPredictor, self).__init__()\n",
        "\n",
        "        # Temporal GCN layer\n",
        "        self.tgcn = TGCN(in_channels=node_features,\n",
        "                         out_channels=hidden_dim)\n",
        "\n",
        "        # Graph attention layer\n",
        "        self.gat = GATConv(\n",
        "            in_channels=hidden_dim,\n",
        "            out_channels=hidden_dim,\n",
        "            heads=num_heads,\n",
        "            concat=True,\n",
        "            dropout=0.3\n",
        "        )\n",
        "\n",
        "        # Prediction layers\n",
        "        self.fc1 = nn.Linear(hidden_dim * num_heads, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 3)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight):\n",
        "        h = None\n",
        "        # Process temporal sequence maintaining hidden state\n",
        "        for t in range(x.size(0)):  # Iterate through time steps\n",
        "            # Select current time step\n",
        "            x_t = x[t, :, :]\n",
        "            edge_index_t = edge_index[t].to(device)\n",
        "            edge_weight_t = edge_weight[t].to(device)\n",
        "            # Update TGCN with hidden state\n",
        "            h = self.tgcn(x_t, edge_index_t, edge_weight_t, H=h)\n",
        "\n",
        "        # Apply GAT only to final hidden state\n",
        "        h = self.gat(h, edge_index_t, edge_weight_t)\n",
        "\n",
        "        # Make predictions\n",
        "        h = F.relu(self.fc1(h))\n",
        "        pred = self.fc2(h)\n",
        "\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s66o8MNuKTKz",
        "outputId": "396006e5-b3f1-4d90-fcba-9b7feb32fdfd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TemporalStockPredictor(\n",
              "  (tgcn): TGCN(\n",
              "    (conv_z): GCNConv(6, 64)\n",
              "    (linear_z): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (conv_r): GCNConv(6, 64)\n",
              "    (linear_r): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (conv_h): GCNConv(6, 64)\n",
              "    (linear_h): Linear(in_features=128, out_features=64, bias=True)\n",
              "  )\n",
              "  (gat): GATConv(64, 64, heads=4)\n",
              "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exceeds Pricing:  0.316635446449119\n",
            "Misses Pricing:  0.28377863222584016\n"
          ]
        }
      ],
      "source": [
        "model = TemporalStockPredictor(\n",
        "    node_features=features_train.shape[-1],\n",
        "    hidden_dim=64,\n",
        "    num_heads=8\n",
        ").to(device)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maXaqF7bKTKy"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_data, test_data, epochs=100, lr=0.001):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    #     optimizer, mode='min', factor=0.5, patience=5\n",
        "    # )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    train_features, train_edges, train_weights = train_data\n",
        "    test_features, test_edges, test_weights = test_data\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        # Process in temporal order with sliding window\n",
        "        for t in tqdm(range(7, len(train_edges)-1)):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get current timestep data\n",
        "            x = train_features[t - 7 :t].to(device)\n",
        "            edge_index = train_edges[t - 7 : t]\n",
        "            edge_weight = train_weights[t - 7 : t]\n",
        "\n",
        "            # Forward pass\n",
        "            pred = model(x, edge_index, edge_weight)\n",
        "\n",
        "            # Calculate loss using next timestep's ternary labels\n",
        "            target = train_features[t, :, -1].to(device).long() + 1 # Last feature is the label\n",
        "            y_one_hot = torch.nn.functional.one_hot(target, 3).to(torch.float64).to(device)\n",
        "            loss = criterion(pred, y_one_hot)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {total_loss/(len(train_features) - 7):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTdtvJgjKTKy",
        "outputId": "1b927efc-588e-4053-c643-9f5ea65689af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 742/1106 [02:40<01:24,  4.32it/s]"
          ]
        }
      ],
      "source": [
        "train_model(model, (features_train, edge_index_train, edge_weights_train), (features_test, edge_index_test, edge_weights_test), epochs=15, lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw7eRALvKTKz"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"tgcnn-0.4-0.4-2000-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "keN-K2jNRkkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8lY8o1ZKTKz"
      },
      "outputs": [],
      "source": [
        "# Evaluate all test data\n",
        "with torch.no_grad():\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    features_test = features_test.to(torch.float32)\n",
        "    edge_index_test = [ei.to(torch.long) for ei in edge_index_test]\n",
        "    edge_weights_test = [ew.to(torch.float32) for ew in edge_weights_test]\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # Generate predictions for each time step\n",
        "    for t in range(7, len(edge_index_test)):\n",
        "        pred = model(\n",
        "            features_test[t-7:t].to(device),\n",
        "            edge_index_test[t-7:t],\n",
        "            edge_weights_test[t-7:t]\n",
        "        )\n",
        "        predictions.append(pred.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njg_tjDTjSoG"
      },
      "outputs": [],
      "source": [
        "# Process predictions\n",
        "predictions = np.stack(predictions)\n",
        "tern_predictions = np.argmax(predictions, axis = 2) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baVv7T9JKTKz"
      },
      "outputs": [],
      "source": [
        "# Load actual percent change and ternary label data\n",
        "prices_pct_change = price_data.pct_change().fillna(0).to_numpy()[-107:]\n",
        "tern_prices = features_test[:, :, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASfnvyukmlDu",
        "outputId": "70ba5bc3-63f2-4709-facf-0242deaadc6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Pct Change:  tensor(0.0161)\n",
            "Binary Counts:  [[  5013.   4924.]\n",
            " [114164. 114316.]\n",
            " [  1018.   2065.]]\n",
            "Mu Std:  [[4.22898633e-03 4.05405002e-03 2.93331714e+00]\n",
            " [1.81876440e-02 1.58791007e-03 1.30161226e+00]\n",
            " [1.02718994e-01 5.45905725e-03 1.58632140e+01]]\n",
            "Counts:  [[ 3230.  3110.  3597.]\n",
            " [66965. 84964. 76551.]\n",
            " [  552.   931.  1600.]]\n"
          ]
        }
      ],
      "source": [
        "# Count predicted ternary labels v. actual ternary labels\n",
        "# Count predicted ternary labels v. actual binary labels\n",
        "# Compute expected return\n",
        "def generate_counts(tern_predictions, tern_prices, prices_pct_change):\n",
        "  counts = np.zeros((3, 3))\n",
        "  bin_counts = np.zeros((3, 2))\n",
        "  evs = [[], [], []]\n",
        "  mu_std = np.zeros((3, 3))\n",
        "  (t, c) = tern_predictions.shape\n",
        "\n",
        "  for i in range(t):\n",
        "    for j in range(c):\n",
        "      # Handle ternary label\n",
        "      counts[int(tern_predictions[i, j] + 1), int(tern_prices[i, j].item() + 1)] += 1\n",
        "\n",
        "      # Handle binary labels\n",
        "      if tern_predictions[i, j] == -1: # We predict it goes down\n",
        "        evs[0].append(prices_pct_change[i, j])\n",
        "        if prices_pct_change[i, j] <= 0: # It actually goes down\n",
        "          bin_counts[0, 0] += 1\n",
        "        else:\n",
        "          bin_counts[0, 1] += 1\n",
        "      elif tern_predictions[i, j] == 1: # We predict it goes up\n",
        "        evs[2].append(prices_pct_change[i, j])\n",
        "        if prices_pct_change[i, j] > 0: # It actually goes up\n",
        "          bin_counts[2, 1] += 1\n",
        "        else:\n",
        "          bin_counts[2, 0] += 1\n",
        "      else:\n",
        "        evs[1].append(prices_pct_change[i, j])\n",
        "        if prices_pct_change[i, j] > 0: # It actually goes up\n",
        "          bin_counts[1, 1] += 1\n",
        "        else:\n",
        "          bin_counts[1, 0] += 1\n",
        "\n",
        "  # Compute expected returns\n",
        "  for i in range(3):\n",
        "    mu_std[i, 0] = np.mean(evs[i])\n",
        "    mu_std[i, 1] = np.std(evs[i]) / np.sqrt(len(evs[i]))\n",
        "    mu_std[i, 2] = abs((mu_std[i, 0] - prices_pct_change.mean().item()) / mu_std[i, 1])\n",
        "\n",
        "  return counts, bin_counts, mu_std, prices_pct_change.mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjJc68ZvkomZ",
        "outputId": "ef01db77-d598-4a91-c284-0fc23d50b02b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Binary Accuracy 0.5436251920122888\n",
            "Overall Accuracy 0.3718178053830228\n",
            "Overall Negative Accuracy 0.5044782127402636\n",
            "Overall Positive Accuracy 0.6698021407719753\n",
            "Negative Recall 0.045655646175809576\n",
            "Approximately Equal Recall 0.9545980562889725\n",
            "Positive Recall 0.019572344277535843\n",
            "Negative Precision 0.32504780114722753\n",
            "Approximately Equal Precision 0.37186624649859945\n",
            "Positive Precision 0.5189750243269543\n"
          ]
        }
      ],
      "source": [
        "def report(counts, bin_counts, mu_std):\n",
        "  actual_tern_sums = np.sum(counts, axis = 0)\n",
        "  pred_sums = np.sum(counts, axis = 1)\n",
        "  actual_bin_sums = np.sum(bin_counts, axis = 0)\n",
        "\n",
        "  print(\"Overall Binary Accuracy\", (bin_counts[0, 0] + bin_counts[2, 1]) / (bin_counts[0, 0] + bin_counts[0, 1] + bin_counts[2, 0] + bin_counts[2, 1]))\n",
        "\n",
        "  print(\"Binary Negative Recall\", (bin_counts[0, 0]) / actual_bin_sums[0])\n",
        "  print(\"Binary Positive Recall\", (bin_counts[2, 1]) / actual_bin_sums[1])\n",
        "\n",
        "  print(\"Binary Negative Precision\", (bin_counts[0, 0]) / pred_sums[0])\n",
        "  print(\"Binary Positive Precision\", (bin_counts[2, 1]) / pred_sums[2])\n",
        "\n",
        "  print(\"Overall Ternary Accuracy\", (counts[0, 0] + counts[1, 1] + counts[2, 2]) / np.sum(counts))\n",
        "\n",
        "  print(\"Ternary Negative Recall\", counts[0, 0] / actual_tern_sums[0])\n",
        "  print(\"Ternary Approximately Equal Recall\", counts[1, 1] / actual_tern_sums[1])\n",
        "  print(\"Ternary Positive Recall\", counts[2, 2] / actual_tern_sums[2])\n",
        "\n",
        "  print(\"Ternary Negative Precision\", counts[0, 0] / pred_sums[0])\n",
        "  print(\"Ternary Approximately Equal Precision\", counts[1, 1] / pred_sums[1])\n",
        "  print(\"Ternary Positive Precision\", counts[2, 2] / pred_sums[2])\n",
        "\n",
        "  print(f\"Negative Expected Return: {mu_std[0, 0]} ± {mu_std[0, 1]}\")\n",
        "  print(f\"Approximately Equal Expected Return: {mu_std[1, 0]} ± {mu_std[1, 1]}\")\n",
        "  print(f\"Positive Expected Return: {mu_std[2, 0]} ± {mu_std[2, 1]}\")\n",
        "\n",
        "  print(f\"Negative rejects Null Hypothesis with t = \", mu_std[0, 2])\n",
        "  print(f\"Approximately Equal rejects Null Hypothesis with t = \", mu_std[1, 2])\n",
        "  print(f\"Positive rejects Null Hypothesis with t = \", mu_std[2, 2])\n",
        "\n",
        "counts, bin_counts, mu_std, mean_change = generate_counts(tern_predictions, tern_prices, prices_pct_change)\n",
        "\n",
        "print(\"Binary Counts: \", bin_counts)\n",
        "print(\"Counts: \", counts)\n",
        "print(\"Average Change: \", mean_change)\n",
        "\n",
        "report(counts, bin_counts, mu_std)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}